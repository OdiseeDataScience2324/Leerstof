{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649c9fc5",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "Bij de use-case anomaly detectie of outlier detectie wordt er op zoek gegaan naar de datapunten die zich niet gedragen zoals de rest van de data. \n",
    "De technieken die hierbinnen vallen kunnen gebruikt worden om fraude te detecteren of om structurele defecten in machines te detecteren om preventief onderhoud in te plannen of in medische gegevens om ziektes te detecteren.\n",
    "\n",
    "Een aparte subset van problemen die binnen dit domein liggen is om hacking of netwerk infiltratie te detecteren.\n",
    "Hier gaat het vaak om een serie van activiteit die verdacht is ipv van enkele datapunten.\n",
    "\n",
    "Een aantal van de meest gebruikte technieken zijn:\n",
    "* isolation forest\n",
    "* Local Outlier Factor\n",
    "* One-class SVM\n",
    "* Neurale netwerken\n",
    "* Hidden Markov Models\n",
    "* ...\n",
    "\n",
    "Anomaly detectie wordt vaak toegepast op ongelabelde data (unsupervised learning) omdat je niet alle zaken die mis kunnen gaan op voorhand kunnen detecteren. \n",
    "Het feit dat deze punten ook zeer zeldzaam zijn maakt het ook moeilijk om er een goeie dataset van op te bouwen.\n",
    "\n",
    "## Isolation Forests\n",
    "\n",
    "De Isolation Forest techniek is een variant op een random forest.\n",
    "Dit betekend dat het een ensemble-techniek is gebaseerd op meerdere beslissingsbomen.\n",
    "In deze techniek wordt de dataset verdeeld door een willekeurige feature te kiezen en deze willekeurig te splitsen tussen het minimum en maximum van de feature.\n",
    "Dit doe je dan voor elke boom tot elk datapunt alleen zit in een eindknoop.\n",
    "De lengte van het pad dat gevolgd moet worden in elke boom is dan een maatstaf voor hoe standaard het punt is.\n",
    "Veel punten die geclusterd zijn worden minder snel gesplitst dan outliers en dus hebben deze een veel langer pad in de boom.\n",
    "Dus datapunten met kortere paden hebben een grote kans om outliers te zijn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate train data\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * rng.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "\n",
    "\n",
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',\n",
    "                 s=20, edgecolor='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',\n",
    "                 s=20, edgecolor='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',\n",
    "                s=20, edgecolor='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144da00",
   "metadata": {},
   "source": [
    "# Local Outlier Factor\n",
    "\n",
    "Dit is een tweede unsupervised outlier detection techniek.\n",
    "Hierbij worden de dichtste buren gebruikt als maatstaf voor hoe dicht een punt tot de standaard behoort.\n",
    "Wanneer dit punt een lage afstand heeft is de kans klein dat het een outlier is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# fit model\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Novelty Detection with LOF\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,\n",
    "                 edgecolors='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,\n",
    "                edgecolors='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"errors novel regular: %d/40 ; errors novel abnormal: %d/40\"\n",
    "    % (n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ff72a",
   "metadata": {},
   "source": [
    "# One Class SVM\n",
    "\n",
    "Standaard SVM zocht naar de beste splitsing tussen twee klassen door middel van een hyperplane dat de marge tussen de klassen zo groot mogelijk maakte.\n",
    "Bij outlier detection is er echter niet geweten welke punten outliers zijn (unsupervised) dus kan het ook niet gebruikt worden om een scheidingslijn te bepalen.\n",
    "One-class SVM is echter een uitbreiding op de standaard SVM maar er wordt gezocht naar een hypersfeer dat alle datapunten omvat. \n",
    "De grootste marge in de context van SMV kan bij de one-class variant geinterpreteerd worden als de kleinst mogelijke hypersfeer.\n",
    "\n",
    "De volgende opmerkingen van standaard SVM gelden nog steeds bij one-class svm:\n",
    "* De marge van de hypersfeer raakt een klein aantal datapunten. Deze worden nog steeds de support-vectors genoemd.\n",
    "* Het is mogelijk om toe te laten dat een aantal punten buiten de sfeer vallen om zo de detectie minder gevoelig voor ruis te maken\n",
    "* Er kan gebruik gemaakt worden van een kernel om de grens complexer te maken dan mogelijk is met de originele features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec458c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# fit the model\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,\n",
    "                 edgecolors='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,\n",
    "                edgecolors='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/40\"\n",
    "    % (n_error_train, n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9fc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
